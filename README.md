This project implements an end-to-end deep learning model that generates descriptive captions for input images using a Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) architecture. The model is trained on the Flickr8k dataset, which contains 8,000 images with five captions each.

The CNN (a pre-trained model, VGG16) is used to extract visual features from images, while the LSTM network learns to generate a corresponding textual description based on these features and previously generated words.
